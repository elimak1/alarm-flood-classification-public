{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, adjusted_rand_score, classification_report\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "methods = [\"sw\", \"nw\", \"lstm\", \"svm\", \"assam\", \"GMM\", \"hmm\", \"castle\", \"tfidf_lr\", \"word2vec\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame()\n",
    "for fold in range(5):\n",
    "    gt = pd.read_csv(f\"../../data/classification/folds/outer_fold_{fold}_test_labels.csv\")\n",
    "    labels = pd.concat([labels, gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = pd.DataFrame(columns=[\"method\", \"fold\", \"accuracy\", \"f1\", \"fdr\", \"mdr\"])\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    for fold in range(5):\n",
    "        gt = pd.read_csv(f\"../../data/classification/folds/outer_fold_{fold}_test_labels.csv\")\n",
    "        gt = gt[\"0\"]\n",
    "        pred = pd.read_csv(f\"../../data/classification/folds/predictions/{method}_outer_fold_{fold}_predictions.csv\")\n",
    "        pred = pred[\"0\"]\n",
    "        accuracy = accuracy_score(gt, pred)\n",
    "        accuracy = np.round(accuracy,5)\n",
    "        f1 = f1_score(gt, pred, average=\"weighted\")\n",
    "        f1 = np.round(f1,5)\n",
    "        e0 = np.sum((gt == -1) & (pred != -1))\n",
    "        e1 = np.sum((gt == -1) & (pred == -1))\n",
    "        fdr = np.round(e0/(e0+e1 + 1e-10),5)\n",
    "\n",
    "        a0 = np.sum((gt != -1) & (pred == -1))\n",
    "        a1 = np.sum((gt != -1) & (pred != -1))\n",
    "        mdr = np.round(a0/(a0+a1), 5)\n",
    "        result_table = result_table.append({\"method\": method, \"fold\": fold, \"accuracy\": accuracy, \"f1\": f1, \"fdr\": fdr, \"mdr\": mdr}, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_results_by_fold = result_table.groupby(\"method\").mean()\n",
    "std_results_by_fold = result_table.groupby(\"method\").std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_results_by_fold.reindex(methods).round(5).to_csv(\"temp.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_95 = mean_results_by_fold - 1.96*std_results_by_fold/np.sqrt(5)\n",
    "higher_95 = mean_results_by_fold + 1.96*std_results_by_fold/np.sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representable_labels = [\"MSW\", \"MNW\", \"LSTM\", \"SVM\", \"ASSAM\", \"GMM\", \"HMM\", \"CASTLE\", \"TF-IDF-LR\", \"DK-Word2Vec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table[\"method\" ]= result_table[\"method\"].apply(lambda x: representable_labels[methods.index(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"accuracy\"]:\n",
    "    plt.figure(figsize=(10, 8))  \n",
    "    sns.set_palette(\"deep\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"paper\")\n",
    "    ax = sns.boxplot(x=result_table[\"method\"],y=result_table[metric])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_xlabel(\"\", fontsize=12)\n",
    "    ax.set_ylabel(metric, fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_flood_result_table = pd.DataFrame(columns=[\"method\", \"fold\", \"minute\", \"accuracy\", \"f1\", \"fdr\", \"mdr\"])\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    for fold in range(5):\n",
    "        gt = pd.read_csv(f\"../../data/classification/folds/outer_fold_{fold}_test_labels.csv\")\n",
    "        gt = gt[\"0\"]\n",
    "        for minute in range(11,21):\n",
    "\n",
    "            pred = pd.read_csv(f\"../../data/classification/online_floods/predictions/{method}/\" + \\\n",
    "                               f\"{method}_outer_fold_{fold}_min_{minute}_predictions.csv\")\n",
    "            pred = pred[\"0\"]\n",
    "            accuracy = accuracy_score(gt, pred)\n",
    "            accuracy = np.round(accuracy,5)\n",
    "            f1 = f1_score(gt, pred, average=\"weighted\")\n",
    "            f1 = np.round(f1,5)\n",
    "            e0 = np.sum((gt == -1) & (pred != -1))\n",
    "            e1 = np.sum((gt == -1) & (pred == -1))\n",
    "            fdr = np.round(e0/(e0+e1 + 1e-10),5)\n",
    "\n",
    "            a0 = np.sum((gt != -1) & (pred == -1))\n",
    "            a1 = np.sum((gt != -1) & (pred != -1))\n",
    "            mdr = np.round(a0/(a0+a1), 5)\n",
    "            online_flood_result_table = online_flood_result_table.append({\"method\": method, \"fold\": fold, \"minute\": minute, \"accuracy\": accuracy, \"f1\": f1, \"fdr\": fdr, \"mdr\": mdr}, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_flood_result_table[\"method\" ]= online_flood_result_table[\"method\"].apply(lambda x: representable_labels[methods.index(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_flood_result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metrics_by_minute = online_flood_result_table.groupby([\"method\", \"minute\"]).mean()\n",
    "mean_metrics_by_minute = mean_metrics_by_minute.reindex(representable_labels, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data=mean_metrics_by_minute.reset_index(), x=\"minute\", y=\"accuracy\", hue=\"method\")\n",
    "ax.set_ylim([0,1])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()\n",
    "ax = sns.lineplot(data=mean_metrics_by_minute.reset_index(), x=\"minute\", y=\"f1\", hue=\"method\")\n",
    "ax.set_ylim([0,1])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()\n",
    "ax = sns.lineplot(data=mean_metrics_by_minute.reset_index(), x=\"minute\", y=\"fdr\", hue=\"method\")\n",
    "ax.set_ylim([0,1])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()\n",
    "ax = sns.lineplot(data=mean_metrics_by_minute.reset_index(), x=\"minute\", y=\"mdr\", hue=\"method\")\n",
    "ax.set_ylim([0,1])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cv paired t-test between accuracies of methods\n",
    "p_value_table = pd.DataFrame(columns=[\"method1\", \"method2\", \"p_value\"])\n",
    "\n",
    "from scipy.stats import ttest_rel\n",
    "for i in range(len(methods)):\n",
    "    for j in range(i+1, len(methods)):\n",
    "        method1 = representable_labels[i]\n",
    "        method2 = representable_labels[j]\n",
    "        t, p = ttest_rel(result_table[result_table[\"method\"] == method1][\"accuracy\"],\n",
    "                            result_table[result_table[\"method\"] == method2][\"accuracy\"])\n",
    "        p_value_table = p_value_table.append({\"method1\": method1, \"method2\": method2, \"p_value\": p}, ignore_index=True)\n",
    "\n",
    "# apply bonferroni correction\n",
    "alpha = 0.05\n",
    "bonferroni_alpha = alpha/len(p_value_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonferroni_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_table[\"significant\"] = p_value_table[\"p_value\"] < bonferroni_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_table[p_value_table[\"significant\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to triu matrix\n",
    "p_value_matrix = p_value_table.pivot(index=\"method1\", columns=\"method2\", values=\"p_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value_matrix.to_csv(\"p_value_matrix.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each class find mean accuracy\n",
    "accuracy_per_class = pd.DataFrame(columns=[\"method\",\"fold\", \"class\", \"accuracy\"])\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    for fold in range(5):\n",
    "        gt = pd.read_csv(f\"../../data/classification/folds/outer_fold_{fold}_test_labels.csv\")\n",
    "        gt = gt[\"0\"]\n",
    "        pred = pd.read_csv(f\"../../data/classification/folds/predictions/{method}_outer_fold_{fold}_predictions.csv\")\n",
    "        pred = pred[\"0\"]\n",
    "        for label in sorted(gt.unique()):\n",
    "            label_indices = gt == label\n",
    "            accuracy = accuracy_score(gt[label_indices], pred[label_indices])\n",
    "            accuracy = np.round(accuracy,5)\n",
    "            accuracy_per_class = accuracy_per_class.append({\"method\": method, \"fold\": fold, \"class\": label, \"accuracy\": accuracy}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_by_cm = accuracy_per_class.groupby([\"class\", \"method\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each method get classes where the method gets top 3 accuracy\n",
    "top_classes = defaultdict(list)\n",
    "worst_classes = defaultdict(list)\n",
    "top_methods = defaultdict(list)\n",
    "worst_methods = defaultdict(list)\n",
    "for c in acc_by_cm[\"class\"].unique():\n",
    "    sor_acc = acc_by_cm[acc_by_cm[\"class\"] == c].sort_values(\"accuracy\", ascending=False)\n",
    "    worst = sor_acc.tail(1)[\"accuracy\"].values\n",
    "    best =  sor_acc.head(1)[\"accuracy\"].values\n",
    "    good = sor_acc[sor_acc[\"accuracy\"].values > best - 0.1]\n",
    "    bad = sor_acc[sor_acc[\"accuracy\"].values < worst + 0.1]\n",
    "    for _,g in good.iterrows():\n",
    "        top_classes[g[\"method\"]].append(c)\n",
    "        top_methods[c].append(g[\"method\"])\n",
    "    for _,b in bad.iterrows():\n",
    "        worst_classes[b[\"method\"]].append(c)\n",
    "        worst_methods[c].append(b[\"method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc_by_class = accuracy_per_class.groupby([\"class\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=mean_acc_by_class.index, y=mean_acc_by_class[\"accuracy\"], edgecolor=\"black\", linewidth=1.5)\n",
    "ax.set_xlabel(\"root cause\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample find mean accuracy\n",
    "accuracy_per_sample = pd.DataFrame(columns=[\"method\",\"fold\", \"sample\", \"correct\", \"predicted_class\", \"true_class\"])\n",
    "for method in methods:\n",
    "    for fold in range(5):\n",
    "        gt = pd.read_csv(f\"../../data/classification/folds/outer_fold_{fold}_test_labels.csv\")\n",
    "        gt = gt[\"0\"]\n",
    "        pred = pd.read_csv(f\"../../data/classification/folds/predictions/{method}_outer_fold_{fold}_predictions.csv\")\n",
    "        pred = pred[\"0\"]\n",
    "        for sample in range(len(gt)):\n",
    "            if gt[sample] == pred[sample]:\n",
    "                correct = 1\n",
    "            else:\n",
    "                correct = 0\n",
    "            accuracy_per_sample = accuracy_per_sample.append({\"method\": method, \"fold\": fold, \"sample\": f\"{fold}_{sample}\", \"correct\": correct, \"predicted_class\": pred[sample], \"true_class\": gt[sample]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sample = accuracy_per_sample.groupby([\"sample\"]).apply(lambda x: np.mean(x[\"correct\"]))\n",
    "by_sample[by_sample < 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_sample[accuracy_per_sample[\"sample\"] == \"3_21\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word2Vec embeddings\n",
    "labels.set_index(\"Unnamed: 0\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alarm_floods = pd.read_csv('../../data/preprocessed/Crane_alarm_floods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import methods.word2vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {str(alarm): i for i, alarm in enumerate(alarm_floods[\"alarmNumber\"].unique())} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = w2v.Word2Vec_Classifier(vocab, 50, 4, lambda_= 1e-08, distance_threshold=0.3)\n",
    "model.fit(alarm_floods, labels, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.plot_embeddings(list(range(0,104)))\n",
    "model.plot_embeddings(list(range(40,50)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
